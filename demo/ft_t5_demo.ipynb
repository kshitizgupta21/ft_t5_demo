{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1704d0af",
   "metadata": {},
   "source": [
    "### Install HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e30b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8a6a2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d868cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "# disable warning in notebook\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333cf514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer \n",
    "from FasterTransformer.examples.pytorch.t5.utils.ft_encoder import FTT5EncoderWeight, FTT5Encoder\n",
    "from FasterTransformer.examples.pytorch.t5.utils.ft_decoding import FTT5DecodingWeight, FTT5Decoding, FTT5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de06884a",
   "metadata": {},
   "source": [
    "## HuggingFace T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94424bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|███████████████████████████████████████████████████████| 1.21k/1.21k [00:00<00:00, 316kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|████████████████████████████████████████████████████████| 242M/242M [00:15<00:00, 16.0MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████████████████████████████████████████████████████| 147/147 [00:00<00:00, 47.5kB/s]\n",
      "Downloading (…)ve/main/spiece.model: 100%|████████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 2.60MB/s]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# specify model name or checkpoint path\n",
    "model_name_or_path = 't5-small'\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(model_name_or_path)\n",
    "t5_model.eval()\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3f2fb",
   "metadata": {},
   "source": [
    "## Define FT T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c22d323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bc12b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"\n",
    "{\n",
    "  \"architectures\": [\n",
    "    \"T5WithLMHeadModel\"\n",
    "  ],\n",
    "  \"d_ff\": 65536,\n",
    "  \"d_kv\": 128,\n",
    "  \"d_model\": 1024,\n",
    "  \"decoder_start_token_id\": 0,\n",
    "  \"dropout_rate\": 0.1,\n",
    "  \"eos_token_id\": 1,\n",
    "  \"initializer_factor\": 1.0,\n",
    "  \"is_encoder_decoder\": true,\n",
    "  \"layer_norm_epsilon\": 1e-06,\n",
    "  \"model_type\": \"t5\",\n",
    "  \"n_positions\": 512,\n",
    "  \"num_heads\": 128,\n",
    "  \"num_layers\": 24,\n",
    "  \"output_past\": true,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"relative_attention_num_buckets\": 32,\n",
    "  \"task_specific_params\": {\n",
    "    \"summarization\": {\n",
    "      \"early_stopping\": true,\n",
    "      \"length_penalty\": 2.0,\n",
    "      \"max_length\": 200,\n",
    "      \"min_length\": 30,\n",
    "      \"no_repeat_ngram_size\": 3,\n",
    "      \"num_beams\": 4,\n",
    "      \"prefix\": \"summarize: \"\n",
    "    },\n",
    "    \"translation_en_to_de\": {\n",
    "      \"early_stopping\": true,\n",
    "      \"max_length\": 300,\n",
    "      \"num_beams\": 4,\n",
    "      \"prefix\": \"translate English to German: \"\n",
    "    },\n",
    "    \"translation_en_to_fr\": {\n",
    "      \"early_stopping\": true,\n",
    "      \"max_length\": 300,\n",
    "      \"num_beams\": 4,\n",
    "      \"prefix\": \"translate English to French: \"\n",
    "    },\n",
    "    \"translation_en_to_ro\": {\n",
    "      \"early_stopping\": true,\n",
    "      \"max_length\": 300,\n",
    "      \"num_beams\": 4,\n",
    "      \"prefix\": \"translate English to Romanian: \"\n",
    "    }\n",
    "  },\n",
    "  \"vocab_size\": 32128\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e63695ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.loads(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f6f0976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architectures': ['T5WithLMHeadModel'],\n",
       " 'd_ff': 65536,\n",
       " 'd_kv': 128,\n",
       " 'd_model': 1024,\n",
       " 'decoder_start_token_id': 0,\n",
       " 'dropout_rate': 0.1,\n",
       " 'eos_token_id': 1,\n",
       " 'initializer_factor': 1.0,\n",
       " 'is_encoder_decoder': True,\n",
       " 'layer_norm_epsilon': 1e-06,\n",
       " 'model_type': 't5',\n",
       " 'n_positions': 512,\n",
       " 'num_heads': 128,\n",
       " 'num_layers': 24,\n",
       " 'output_past': True,\n",
       " 'pad_token_id': 0,\n",
       " 'relative_attention_num_buckets': 32,\n",
       " 'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "   'length_penalty': 2.0,\n",
       "   'max_length': 200,\n",
       "   'min_length': 30,\n",
       "   'no_repeat_ngram_size': 3,\n",
       "   'num_beams': 4,\n",
       "   'prefix': 'summarize: '},\n",
       "  'translation_en_to_de': {'early_stopping': True,\n",
       "   'max_length': 300,\n",
       "   'num_beams': 4,\n",
       "   'prefix': 'translate English to German: '},\n",
       "  'translation_en_to_fr': {'early_stopping': True,\n",
       "   'max_length': 300,\n",
       "   'num_beams': 4,\n",
       "   'prefix': 'translate English to French: '},\n",
       "  'translation_en_to_ro': {'early_stopping': True,\n",
       "   'max_length': 300,\n",
       "   'num_beams': 4,\n",
       "   'prefix': 'translate English to Romanian: '}},\n",
       " 'vocab_size': 32128}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c6e75d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"t5-small\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"relu\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": false,\n",
       "  \"is_gated_act\": false,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 6,\n",
       "  \"num_heads\": 8,\n",
       "  \"num_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 30,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"summarize: \"\n",
       "    },\n",
       "    \"translation_en_to_de\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to German: \"\n",
       "    },\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to French: \"\n",
       "    },\n",
       "    \"translation_en_to_ro\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to Romanian: \"\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bce3b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = t5_model.encoder.config\n",
    "decoder_config = t5_model.decoder.config\n",
    "activation_type = encoder_config.feed_forward_proj\n",
    "tie_word_embeddings = decoder_config.tie_word_embeddings\n",
    "\n",
    "# single-gpu so set TP=1, PP=1\n",
    "tensor_para_size = 1\n",
    "pipeline_para_size = 1\n",
    "t5_with_bias = False\n",
    "use_gated_activation = False\n",
    "position_embedding_type = 0\n",
    "weight_data_type = np.float32\n",
    "q_scaling = 1.0 / (math.sqrt(encoder_config.d_kv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dcb1113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] WARNING: Exception occurred in dist.init_process_group(backend = 'mpi'). Maybe the process group has been initialized somewhere else.\n"
     ]
    }
   ],
   "source": [
    "ft_encoder_weight = FTT5EncoderWeight(\n",
    "    encoder_config,\n",
    "    tensor_para_size,\n",
    "    pipeline_para_size,\n",
    "    t5_with_bias=t5_with_bias,\n",
    "    use_gated_activation=use_gated_activation,\n",
    "    position_embedding_type=position_embedding_type,\n",
    "    weight_data_type=weight_data_type,\n",
    ")\n",
    "ft_decoding_weight = FTT5DecodingWeight(\n",
    "    decoder_config,\n",
    "    tensor_para_size,\n",
    "    pipeline_para_size,\n",
    "    t5_with_bias=t5_with_bias,\n",
    "    use_gated_activation=use_gated_activation,\n",
    "    position_embedding_type=position_embedding_type,\n",
    "    weight_data_type=weight_data_type,\n",
    ")\n",
    "\n",
    "ft_encoder_weight.load_from_model(t5_model)\n",
    "ft_decoding_weight.load_from_model(t5_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a6f316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data_type = \"fp16\"\n",
    "\n",
    "if inference_data_type == \"fp32\":\n",
    "    ft_encoder_weight.to_float()\n",
    "    ft_decoding_weight.to_float()\n",
    "elif inference_data_type == \"fp16\":\n",
    "    ft_encoder_weight.to_half()\n",
    "    ft_decoding_weight.to_half()\n",
    "elif inference_data_type == \"bf16\":\n",
    "    ft_encoder_weight.to_bfloat16()\n",
    "    ft_decoding_weight.to_bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "040401b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] WARNING: Exception occurred in dist.init_process_group(backend = 'mpi'). Maybe the process group has been initialized somewhere else.\n",
      "[FT][WARNING] Skip NCCL initialization since requested tensor/pipeline parallel sizes are equals to 1.\n",
      "[WARNING] gemm_config.in is not found; using default GEMM algo\n",
      "[INFO] WARNING: Exception occurred in dist.init_process_group(backend = 'mpi'). Maybe the process group has been initialized somewhere else.\n",
      "[FT][WARNING] Skip NCCL initialization since requested tensor/pipeline parallel sizes are equals to 1.\n",
      "[WARNING] gemm_config.in is not found; using default GEMM algo\n"
     ]
    }
   ],
   "source": [
    "remove_padding = False\n",
    "max_distance = 128\n",
    "sparse = False\n",
    "lib_path = './FasterTransformer/build/lib/libth_transformer.so'\n",
    "\n",
    "ft_encoder = FTT5Encoder(ft_encoder_weight.w,\n",
    "                         lib_path,\n",
    "                         encoder_config.num_heads,\n",
    "                         encoder_config.d_kv,\n",
    "                         encoder_config.d_ff,\n",
    "                         encoder_config.d_model,\n",
    "                         remove_padding,\n",
    "                         encoder_config.num_layers,\n",
    "                         encoder_config.relative_attention_num_buckets,\n",
    "                         0, # num_experts\n",
    "                         [], # moe_layer_index\n",
    "                         max_distance,\n",
    "                         sparse,\n",
    "                         q_scaling,\n",
    "                         tensor_para_size,\n",
    "                         pipeline_para_size,\n",
    "                         t5_with_bias,\n",
    "                         position_embedding_type,\n",
    "                         activation_type=activation_type)\n",
    "\n",
    "ft_decoding = FTT5Decoding(ft_decoding_weight.w,\n",
    "                           lib_path,\n",
    "                           decoder_config.num_heads,\n",
    "                           decoder_config.d_kv,\n",
    "                           decoder_config.d_ff,\n",
    "                           encoder_config.d_model,\n",
    "                           decoder_config.d_model,\n",
    "                           decoder_config.num_layers,\n",
    "                           decoder_config.decoder_start_token_id,\n",
    "                           decoder_config.eos_token_id,\n",
    "                           decoder_config.vocab_size,\n",
    "                           q_scaling,\n",
    "                           decoder_config.relative_attention_num_buckets,\n",
    "                           0, # num_experts\n",
    "                           [], # moe_layer_index,\n",
    "                           max_distance,\n",
    "                           tensor_para_size=tensor_para_size,\n",
    "                           pipeline_para_size=pipeline_para_size,\n",
    "                           t5_with_bias=t5_with_bias,\n",
    "                           position_embedding_type=position_embedding_type,\n",
    "                           activation_type=activation_type,\n",
    "                           tie_word_embeddings=tie_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c04f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_t5 = FTT5(ft_encoder, ft_decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71555c",
   "metadata": {},
   "source": [
    "## Define Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aac7dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS = [\n",
    "    \"translate English to French: Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems\",\n",
    "    \"translate English to German: Giant sequoia trees are the largest trees by volume in the world\"\n",
    "]\n",
    "batch_size = len(INPUTS)\n",
    "inputs = tokenizer(INPUTS, padding=True, return_tensors=\"pt\")\n",
    "input_ids = inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d7e1a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76169276",
   "metadata": {},
   "source": [
    "## Set generation settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ba378b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set output len to 64\n",
    "max_seq_len = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688dc84e",
   "metadata": {},
   "source": [
    "## HF Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90a764aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"L'intelligence artificielle est la simulation des processus de l'intelligence humaine par des machines, en particulier des systèmes informatiques.\",\n",
       " 'Riesensequoien sind die größten Baumarten weltweit']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = t5_model.generate(input_ids, max_length=max_seq_len)\n",
    "hf_tokens = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "hf_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe5ee6",
   "metadata": {},
   "source": [
    "## FT Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "666b0493",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_diversity_rate = 0.0\n",
    "# beam width\n",
    "num_beams = 1\n",
    "# topk and topp sampling\n",
    "topk = 0\n",
    "topp = 0.0\n",
    "\n",
    "# An example to prevent generating \"Chef\"\n",
    "# bad_words_text = np.array([[\"Chef\"]]* len(input_texts), dtype=object)\n",
    "# bad_words_list = to_word_list_format(bad_words_text, tokenizer)\n",
    "# bad_words_list = torch.Tensor(bad_words_list).to(torch.int32).to(\"cuda\").contiguous()\n",
    "bad_words_list = None\n",
    "\n",
    "# An example to stop generation when the model generate \"Chef\"\n",
    "# stop_words_text = np.array([[\"Chef\"]] * len(input_texts), dtype=object)\n",
    "# stop_words_list = to_word_list_format(stop_words_text, tokenizer)\n",
    "# stop_words_list = torch.Tensor(stop_words_list).to(torch.int32).to(\"cuda\").contiguous()\n",
    "stop_words_list = None\n",
    "\n",
    "repetition_penalty = 1.0\n",
    "temperature = 1.0\n",
    "len_penalty = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fadbef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_t5 returns output_ids of shape [batch_size, beam_width, max_output_seq_len]\n",
    "# ft_t5 returns sequence_length of shape [batch_size, beam_width]\n",
    "ft_output_ids, ft_sequence_length = ft_t5(input_token=inputs,\n",
    "                                                  inputs_embeds=None,\n",
    "                                                  beam_size=num_beams,\n",
    "                                                  max_seq_len=max_seq_len,\n",
    "                                                  top_k=topk,\n",
    "                                                  top_p=topp,\n",
    "                                                  beam_search_diversity_rate=beam_search_diversity_rate,\n",
    "                                                  is_return_output_log_probs=False,\n",
    "                                                  is_return_cum_log_probs=False,\n",
    "                                                  repetition_penalty=repetition_penalty,\n",
    "                                                  temperature=temperature,\n",
    "                                                  len_penalty=len_penalty,\n",
    "                                                  bad_words_list=bad_words_list,\n",
    "                                                  stop_words_list=stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99803621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"L'intelligence artificielle est la simulation des processus de l'intelligence humaine par des machines, en particulier des systèmes informatiques.\",\n",
       " 'Riesensequoien sind die größten Baumarten weltweit']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_outputs = []\n",
    "for i in range(batch_size):\n",
    "    # selecting the top sequence from beam width number of sequences\n",
    "    ft_outputs.append(list(ft_output_ids[i, 0, :][:ft_sequence_length[i , 0]]))\n",
    "ft_tokens = tokenizer.batch_decode(ft_outputs, skip_special_tokens=True)\n",
    "\n",
    "ft_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2dad7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
