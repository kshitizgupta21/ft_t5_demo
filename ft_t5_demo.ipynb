{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1749c97",
   "metadata": {},
   "source": [
    "### Install HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8abe17b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers[sentencepiece]\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (2022.10.31)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
      "\u001b[K     |████████████████████████████████| 190 kB 127.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (22.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 112.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (2.28.1)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 121.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[sentencepiece]) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (1.26.13)\n",
      "Installing collected packages: filelock, tokenizers, huggingface-hub, transformers, sentencepiece\n",
      "Successfully installed filelock-3.9.0 huggingface-hub-0.12.0 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441de1a9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818316a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0502359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "# disable warning in notebook\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06d5871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer \n",
    "from FasterTransformer.examples.pytorch.t5.utils.ft_encoder import FTT5EncoderWeight, FTT5Encoder\n",
    "from FasterTransformer.examples.pytorch.t5.utils.ft_decoding import FTT5DecodingWeight, FTT5Decoding, FTT5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4997e16",
   "metadata": {},
   "source": [
    "## Set HuggingFace T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f8e502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|███████████████████████████████████████████████████████| 1.21k/1.21k [00:00<00:00, 290kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|█████████████████████████████████████████████████████████| 892M/892M [00:04<00:00, 204MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████████████████████████████████████████████████████| 147/147 [00:00<00:00, 46.5kB/s]\n",
      "Downloading (…)ve/main/spiece.model: 100%|████████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 2.56MB/s]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# specify model name or checkpoint path\n",
    "model_name_or_path = 't5-base'\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(model_name_or_path)\n",
    "t5_model.eval()\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ce3e20",
   "metadata": {},
   "source": [
    "## Set FT T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "771f1b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = t5_model.encoder.config\n",
    "decoder_config = t5_model.decoder.config\n",
    "encoder_config.update({\"num_experts\": 0})\n",
    "decoder_config.update({\"num_experts\": 0})\n",
    "encoder_config.update({\"moe_layer_index\": []})\n",
    "decoder_config.update({\"moe_layer_index\": []})\n",
    "activation_type = encoder_config.feed_forward_proj\n",
    "tie_word_embeddings = decoder_config.tie_word_embeddings\n",
    "\n",
    "# single-gpu so set TP=1, PP=1\n",
    "tensor_para_size = 1\n",
    "pipeline_para_size = 1\n",
    "t5_with_bias = False\n",
    "use_gated_activation = False\n",
    "position_embedding_type = 0\n",
    "weight_data_type = np.float32\n",
    "q_scaling = 1.0 / (math.sqrt(encoder_config.d_kv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfbe07c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] WARNING: Exception occurred in dist.init_process_group(backend = 'mpi'). Maybe the process group has been initialized somewhere else.\n",
      "[INFO] WARNING: Exception occurred in dist.init_process_group(backend = 'mpi'). Maybe the process group has been initialized somewhere else.\n"
     ]
    }
   ],
   "source": [
    "ft_encoder_weight = FTT5EncoderWeight(\n",
    "    encoder_config,\n",
    "    tensor_para_size,\n",
    "    pipeline_para_size,\n",
    "    t5_with_bias=t5_with_bias,\n",
    "    use_gated_activation=use_gated_activation,\n",
    "    position_embedding_type=position_embedding_type,\n",
    "    weight_data_type=weight_data_type,\n",
    ")\n",
    "ft_decoding_weight = FTT5DecodingWeight(\n",
    "    decoder_config,\n",
    "    tensor_para_size,\n",
    "    pipeline_para_size,\n",
    "    t5_with_bias=t5_with_bias,\n",
    "    use_gated_activation=use_gated_activation,\n",
    "    position_embedding_type=position_embedding_type,\n",
    "    weight_data_type=weight_data_type,\n",
    ")\n",
    "\n",
    "ft_encoder_weight.load_from_model(t5_model)\n",
    "ft_decoding_weight.load_from_model(t5_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80db5c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_fp16= True\n",
    "if use_fp16:\n",
    "    ft_encoder_weight.to_half()\n",
    "    ft_decoding_weight.to_half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7326669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] WARNING: Exception occurred in dist.init_process_group(backend = 'mpi'). Maybe the process group has been initialized somewhere else.\n",
      "[INFO] WARNING: Exception occurred in dist.init_process_group(backend = 'mpi'). Maybe the process group has been initialized somewhere else.\n",
      "[FT][WARNING] Skip NCCL initialization since requested tensor/pipeline parallel sizes are equals to 1.\n",
      "[WARNING] gemm_config.in is not found; using default GEMM algo\n",
      "[FT][WARNING] Skip NCCL initialization since requested tensor/pipeline parallel sizes are equals to 1.\n",
      "[WARNING] gemm_config.in is not found; using default GEMM algo\n"
     ]
    }
   ],
   "source": [
    "remove_padding = False\n",
    "max_distance = 128\n",
    "sparse = False\n",
    "lib_path = './FasterTransformer/build/lib/libth_transformer.so'\n",
    "ft_encoder = FTT5Encoder(ft_encoder_weight.w, lib_path, encoder_config.num_heads,\n",
    "                        encoder_config.d_kv, encoder_config.d_ff,\n",
    "                        encoder_config.d_model, remove_padding, encoder_config.num_layers,\n",
    "                        encoder_config.relative_attention_num_buckets, encoder_config.num_experts, encoder_config.moe_layer_index,\n",
    "                        max_distance, sparse, q_scaling, tensor_para_size, pipeline_para_size, t5_with_bias,\n",
    "                        position_embedding_type,\n",
    "                        activation_type=activation_type,)\n",
    "ft_decoding = FTT5Decoding(ft_decoding_weight.w, lib_path,\n",
    "                        decoder_config.num_heads, decoder_config.d_kv,\n",
    "                        decoder_config.d_ff, encoder_config.d_model,\n",
    "                        decoder_config.d_model, decoder_config.num_layers,\n",
    "                        decoder_config.decoder_start_token_id, decoder_config.eos_token_id,\n",
    "                        decoder_config.vocab_size,\n",
    "                        q_scaling,\n",
    "                        decoder_config.relative_attention_num_buckets, decoder_config.num_experts, decoder_config.moe_layer_index, max_distance,\n",
    "                        tensor_para_size=tensor_para_size, pipeline_para_size=pipeline_para_size,\n",
    "                        t5_with_bias=t5_with_bias,\n",
    "                        position_embedding_type=position_embedding_type,\n",
    "                        activation_type=activation_type, tie_word_embeddings=tie_word_embeddings,)\n",
    "\n",
    "ft_t5 = FTT5(ft_encoder, ft_decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39030763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat sample input 3 times to get to 150 input tokens length\n",
    "INPUTS = [\n",
    "    \"translate English to French: Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems\"\n",
    "]\n",
    "batch_size = len(INPUTS)\n",
    "inputs = tokenizer(INPUTS, padding=True, return_tensors=\"pt\")\n",
    "input_ids = inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9c43a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877707b",
   "metadata": {},
   "source": [
    "## Set beam width and max length and other settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68ba0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set output len to 60\n",
    "max_seq_len = 60\n",
    "beam_search_diversity_rate = 0.0\n",
    "# beam width\n",
    "num_beams = 2\n",
    "# topk and topp sampling\n",
    "topk = 0\n",
    "topp = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc908a4",
   "metadata": {},
   "source": [
    "## HF Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04e3d015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"L'intelligence artificielle est la simulation des processus de l'intelligence humaine par des machines, en particulier des systèmes informatiques.\"]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = t5_model.generate(input_ids, max_length=max_seq_len, num_beams=num_beams)\n",
    "hf_tokens = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "hf_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b134e",
   "metadata": {},
   "source": [
    "## FT Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa4af851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example to prevent generating \"Chef\"\n",
    "# bad_words_text = np.array([[\"Chef\"]]* len(input_texts), dtype=object)\n",
    "# bad_words_list = to_word_list_format(bad_words_text, tokenizer)\n",
    "# bad_words_list = torch.Tensor(bad_words_list).to(torch.int32).to(\"cuda\").contiguous()\n",
    "bad_words_list = None\n",
    "\n",
    "# An example to stop generation when the model generate \"Chef\"\n",
    "# stop_words_text = np.array([[\"Chef\"]] * len(input_texts), dtype=object)\n",
    "# stop_words_list = to_word_list_format(stop_words_text, tokenizer)\n",
    "# stop_words_list = torch.Tensor(stop_words_list).to(torch.int32).to(\"cuda\").contiguous()\n",
    "stop_words_list = None\n",
    "\n",
    "repetition_penalty = 1.0\n",
    "temperature = 1.0\n",
    "len_penalty = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1f45bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_t5 returns output_ids of shape [batch_size, beam_width, max_output_seq_len]\n",
    "# ft_t5 returns sequence_length of shape [batch_size, beam_width]\n",
    "ft_output_ids, ft_sequence_length = ft_t5(input_token=inputs,\n",
    "                                                  inputs_embeds=None,\n",
    "                                                  beam_size=num_beams,\n",
    "                                                  max_seq_len=max_seq_len,\n",
    "                                                  top_k=topk,\n",
    "                                                  top_p=topp,\n",
    "                                                  beam_search_diversity_rate=beam_search_diversity_rate,\n",
    "                                                  is_return_output_log_probs=False,\n",
    "                                                  is_return_cum_log_probs=False,\n",
    "                                                  repetition_penalty=repetition_penalty,\n",
    "                                                  temperature=temperature,\n",
    "                                                  len_penalty=len_penalty,\n",
    "                                                  bad_words_list=bad_words_list,\n",
    "                                                  stop_words_list=stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8610afce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"L'intelligence artificielle est la simulation des processus de l'intelligence humaine par des machines, en particulier des systèmes informatiques.\"]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_outputs = []\n",
    "for i in range(batch_size):\n",
    "    # selecting the top sequence from beam width number of sequences\n",
    "    ft_outputs.append(list(ft_output_ids[i, 0, :][:ft_sequence_length[i , 0]]))\n",
    "ft_tokens = tokenizer.batch_decode(ft_outputs, skip_special_tokens=True)\n",
    "\n",
    "ft_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
